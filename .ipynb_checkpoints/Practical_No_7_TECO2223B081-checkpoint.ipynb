{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b7f8057a",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'nltk'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mstring\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mre\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'nltk'"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import string\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2279d80e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_lowercase(text):\n",
    "    return text.lower()\n",
    "\n",
    "input_str=\"Hi My Name is Dnyanesh Bachhav\"\n",
    "text_lowercase(input_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6c39a3e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello my Name is Yadnesh Wani and My Age is '"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def remove_numbers(text):\n",
    "    result=re.sub(r'\\d+','',text)\n",
    "    return result\n",
    "\n",
    "input_str=\"Hello my Name is Dnyanesh Bachhav and My Age is 20\"\n",
    "remove_numbers(input_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "39d0be7b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'What is Your Name  My Name is Yadnesh  '"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def remove_punctuation(text):\n",
    "    translator=str.maketrans('','',string.punctuation)\n",
    "    return text.translate(translator)\n",
    "\n",
    "input_str=\"What is Your Name ? My Name is Yadnesh ! \"\n",
    "remove_punctuation(input_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a43a5e8c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello World !'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def remove_whitespace(text):\n",
    "    return \" \".join(text.split())\n",
    "\n",
    "input_str=\"Hello       World    !\"\n",
    "remove_whitespace(input_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7db196b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\acer\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\acer\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['This', 'sample', 'sentence', 'going', 'remove', 'stopwords']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    stop_words=set(stopwords.words(\"english\"))\n",
    "    word_token=word_tokenize(text)\n",
    "    filtered_text=[word for word in word_token if word not in stop_words]\n",
    "    return filtered_text\n",
    "\n",
    "example_text=\"This is a sample sentence and we are going to remove the stopwords\"\n",
    "remove_stopwords(example_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b041c042",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['data',\n",
       " 'scienc',\n",
       " 'use',\n",
       " 'scientif',\n",
       " 'method',\n",
       " 'algorithm',\n",
       " 'and',\n",
       " 'mani',\n",
       " 'type',\n",
       " 'of',\n",
       " 'process']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "stemmer=PorterStemmer()\n",
    "\n",
    "\n",
    "def stem_words(text):\n",
    "    word_tokens=word_tokenize(text)\n",
    "    stems=[stemmer.stem(word) for word in word_tokens]\n",
    "    return stems\n",
    "\n",
    "text=\"Data Science use Scientific methods algorithms and many types of Process\"\n",
    "stem_words(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8cb577cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\acer\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\acer\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Data',\n",
       " 'Science',\n",
       " 'use',\n",
       " 'Scientific',\n",
       " 'methods',\n",
       " 'algorithms',\n",
       " 'and',\n",
       " 'many',\n",
       " 'type',\n",
       " 'of',\n",
       " 'Process']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "lemmatizer=WordNetLemmatizer()\n",
    "\n",
    "\n",
    "def lemmatize_word(text):\n",
    "    word_tokens=word_tokenize(text)\n",
    "    lemmas=[lemmatizer.lemmatize(word,pos='v') for word in word_tokens]\n",
    "    return lemmas\n",
    "\n",
    "text=\"Data Science use Scientific methods algorithms and many types of Process\"\n",
    "lemmatize_word(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18a822d6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd2eac19",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51948ea8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9bc043e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sklearn as sk\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dbae6c55",
   "metadata": {},
   "outputs": [],
   "source": [
    "first_sentence=\"Data Science is one of the top job of the 21st century\"\n",
    "second_sentence=\"Machine Learning is the key for Data Science\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "208646b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'the', 'key', 'for', 'Data', '21st', 'Science', 'century', 'of', 'Learning', 'Machine', 'one', 'job', 'top', 'is'}\n"
     ]
    }
   ],
   "source": [
    "first_sentence=first_sentence.split(\" \")\n",
    "second_sentence=second_sentence.split(\" \")\n",
    "\n",
    "total=set(first_sentence).union(set(second_sentence))\n",
    "print(total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fb590668",
   "metadata": {},
   "outputs": [],
   "source": [
    "wordDictA=dict.fromkeys(total,0)\n",
    "wordDictB=dict.fromkeys(total,0)\n",
    "for word in first_sentence:\n",
    "    wordDictA[word]+=1\n",
    "\n",
    "for word in second_sentence:\n",
    "    wordDictB[word]+=1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c7d9d250",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>the</th>\n",
       "      <th>key</th>\n",
       "      <th>for</th>\n",
       "      <th>Data</th>\n",
       "      <th>21st</th>\n",
       "      <th>Science</th>\n",
       "      <th>century</th>\n",
       "      <th>of</th>\n",
       "      <th>Learning</th>\n",
       "      <th>Machine</th>\n",
       "      <th>one</th>\n",
       "      <th>job</th>\n",
       "      <th>top</th>\n",
       "      <th>is</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   the  key  for  Data  21st  Science  century  of  Learning  Machine  one  \\\n",
       "0    2    0    0     1     1        1        1   2         0        0    1   \n",
       "1    1    1    1     1     0        1        0   0         1        1    0   \n",
       "\n",
       "   job  top  is  \n",
       "0    1    1   1  \n",
       "1    0    0   1  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame([wordDictA,wordDictB])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b35b355e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeTF(wordDict,doc):\n",
    "    tfDict={}\n",
    "    corpusCount=len(doc)\n",
    "    for word,count in wordDict.items():\n",
    "        tfDict[word]=count/float(corpusCount)\n",
    "    return(tfDict)\n",
    "\n",
    "tfFirst=computeTF(wordDictA,first_sentence)\n",
    "tfSecond=computeTF(wordDictB,second_sentence)\n",
    "\n",
    "tf=pd.DataFrame([tfFirst,tfSecond])\n",
    "                  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "92ed928c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>the</th>\n",
       "      <th>key</th>\n",
       "      <th>for</th>\n",
       "      <th>Data</th>\n",
       "      <th>21st</th>\n",
       "      <th>Science</th>\n",
       "      <th>century</th>\n",
       "      <th>of</th>\n",
       "      <th>Learning</th>\n",
       "      <th>Machine</th>\n",
       "      <th>one</th>\n",
       "      <th>job</th>\n",
       "      <th>top</th>\n",
       "      <th>is</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.083333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.125000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        the    key    for      Data      21st   Science   century        of  \\\n",
       "0  0.166667  0.000  0.000  0.083333  0.083333  0.083333  0.083333  0.166667   \n",
       "1  0.125000  0.125  0.125  0.125000  0.000000  0.125000  0.000000  0.000000   \n",
       "\n",
       "   Learning  Machine       one       job       top        is  \n",
       "0     0.000    0.000  0.083333  0.083333  0.083333  0.083333  \n",
       "1     0.125    0.125  0.000000  0.000000  0.000000  0.125000  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c070ab75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['key', 'Data', '21st', 'Science', 'century', 'Learning', 'Machine', 'one', 'job', 'top']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\acer\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "stop_words=set(stopwords.words('english'))\n",
    "filtered_sentence=[w for w in wordDictA if not w in stop_words]\n",
    "print(filtered_sentence)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "18fe89d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeIDF(docList):\n",
    "    idfDict={}\n",
    "    N=len(docList)\n",
    "    \n",
    "    idfDict=dict.fromkeys(docList[0].keys(),0)\n",
    "    for word,val in idfDict.items():\n",
    "        idfDict[word]=math.log10(N/(float(val)+1))\n",
    "        \n",
    "    return(idfDict)\n",
    "idfs=computeIDF([wordDictA,wordDictB])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0a88cfd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        the       key       for      Data      21st   Science   century  \\\n",
      "0  0.050172  0.000000  0.000000  0.025086  0.025086  0.025086  0.025086   \n",
      "1  0.037629  0.037629  0.037629  0.037629  0.000000  0.037629  0.000000   \n",
      "\n",
      "         of  Learning   Machine       one       job       top        is  \n",
      "0  0.050172  0.000000  0.000000  0.025086  0.025086  0.025086  0.025086  \n",
      "1  0.000000  0.037629  0.037629  0.000000  0.000000  0.000000  0.037629  \n"
     ]
    }
   ],
   "source": [
    "def computeTFIDF(tfBow,idfs):\n",
    "    tfidf={}\n",
    "    for word,val in tfBow.items():\n",
    "        tfidf[word]=val*idfs[word]\n",
    "    return(tfidf)\n",
    "\n",
    "idfFirst=computeTFIDF(tfFirst,idfs)\n",
    "idfSecond=computeTFIDF(tfSecond,idfs)\n",
    "\n",
    "idf=pd.DataFrame([idfFirst,idfSecond])\n",
    "print(idf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "446fbbd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "firstV = \"Data Science is one of the top job of 21st Century\"\n",
    "secondV= \"Machine Learning is the key for data science\"\n",
    "\n",
    "vectorize=TfidfVectorizer()\n",
    "\n",
    "response=vectorize.fit_transform([firstV,secondV])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "5526423f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 1)\t0.3011696304075596\n",
      "  (0, 0)\t0.3011696304075596\n",
      "  (0, 5)\t0.3011696304075596\n",
      "  (0, 13)\t0.3011696304075596\n",
      "  (0, 12)\t0.21428467250457112\n",
      "  (0, 9)\t0.6023392608151192\n",
      "  (0, 10)\t0.3011696304075596\n",
      "  (0, 4)\t0.21428467250457112\n",
      "  (0, 11)\t0.21428467250457112\n",
      "  (0, 2)\t0.21428467250457112\n",
      "  (1, 3)\t0.40740123733358447\n",
      "  (1, 6)\t0.40740123733358447\n",
      "  (1, 7)\t0.40740123733358447\n",
      "  (1, 8)\t0.40740123733358447\n",
      "  (1, 12)\t0.28986933576883284\n",
      "  (1, 4)\t0.28986933576883284\n",
      "  (1, 11)\t0.28986933576883284\n",
      "  (1, 2)\t0.28986933576883284\n"
     ]
    }
   ],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72f44619",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
